# serengeti bootup configurations, updated by firstboot script
serengeti.uuid = 1234

# root vm folder for all clusters will be SERENGETI-CLUSTER-${serengeti.uuid}
serengeti.root_vm_folder_prefix = SERENGETI-CLUSTER

# Turn on intensive checks in debug mode (including AuAssert checks)
# Note: the debug code should not have side-effect on the outside code,
# i.e. turning off debug should not leads to changes of code logic
serengeti.debug = true

# DAL transaction random rollback, i.e. deadlock simulation
# only valid when serengeti.debug = true
dal.stressTxnRollback = true

vc_datacenter = datacenter
template_id = vm-001

serengeti.distro_root = http://localhost/distros

create_cluster.cmd = sudo -u serengeti /home/serengeti/aurora_bigdata/distribute/sbin/ironfan_proxy.sh create :cluster_name :json_file
update_cluster.cmd = sudo -u serengeti /home/serengeti/aurora_bigdata/distribute/sbin/ironfan_proxy.sh update :cluster_name :json_file
start_cluster_node.cmd = sudo -u serengeti /home/serengeti/aurora_bigdata/distribute/sbin/ironfan_proxy.sh start :nodes_name :json_file
stop_cluster_node.cmd = sudo -u serengeti /home/serengeti/aurora_bigdata/distribute/sbin/ironfan_proxy.sh stop :nodes_name :json_file
delete_cluster.cmd = sudo -u serengeti /home/serengeti/aurora_bigdata/distribute/sbin/ironfan_proxy.sh destroy :cluster_name :json_file
kill_task.cmd = sudo -u serengeti /home/serengeti/aurora_bigdata/distribute/sbin/kill_task.sh
configure_cluster.cmd = sudo -u serengeti /home/serengeti/aurora_bigdata/distribute/sbin/ironfan_proxy.sh configure :cluster_name :json_file

# task configurations
task.enable_mq = true
task.threadpool.workers = 20
task.threadpool.queue_size = 50
task.rabbitmq.host = localhost
task.rabbitmq.port = 5672
task.rabbitmq.username =
task.rabbitmq.password =
task.rabbitmq.exchange = bddtask
task.rabbitmq.routekey_fmt = task.${task_id}
task.rabbitmq.recv_timeout_ms = 1000
task.rabbitmq.keepalive_time_ms = 5000

# storage size configuration (GB)
# master group represents the group contains hadoop_namenode or hadoop_jobtracker role.
# worker group represents the group contains hadoop_datanode or hadoop_tasktracker role.
# client group represents the group contains hadoop_client or pig, hive role.
# the last field represents the instance type, XL, L, M, and S.
storage.mastergroup.extralarge = 200
storage.mastergroup.large = 100
storage.mastergroup.medium = 50
storage.mastergroup.small = 25
storage.workergroup.extralarge = 400
storage.workergroup.large = 200
storage.workergroup.medium = 100
storage.workergroup.small = 50
storage.clientgroup.extralarge = 400
storage.clientgroup.large = 200
storage.clientgroup.medium = 100
storage.clientgroup.small = 50
