# serengeti bootup configurations, updated by firstboot script
serengeti.uuid = xxx-uuid
serengeti.initialize.uuid = true

# root vm folder for all clusters will be SERENGETI-CLUSTER-${serengeti.uuid}
serengeti.root_folder_prefix = SERENGETI-vApp

# Turn on intensive checks in debug mode (including AuAssert checks)
# Note: the debug code should not have side-effect on the outside code,
# i.e. turning off debug should not leads to changes of code logic
serengeti.debug = true

# the max number of event processors to handle vCenter events
serengeti.event_processor.poolsize = 8

# the number of concurrent clone a template should support
serengeti.singlevm.concurrency = 1

# also means max number of nodes a cluster can have
serengeti.scheduler.poolsize = 1024

# version field
serengeti.version = 2.3.0

# DAL transaction random rollback, i.e. deadlock simulation
# only valid when serengeti.debug = true
dal.stressTxnRollback = true

# Is Serengeti deployed as an bundle vApp or two separeted VMs?
deploy_as_vapp = true

# initialize serengeti server with reousrces from the serengeti server VM?
# include the datastore, resource pool, network
init_resource = false

# upgrade flag
serengeti.just_upgraded = false

# Turn on http proxy if the VMs in the cluster created by Serengeti Server need an http proxy to connect to the Internet.
# The wildcard doesn't work for 'serengeti.no_proxy'
#serengeti.http_proxy = http://proxy.domain.com:port
#serengeti.no_proxy = xyz.domain.com, 10.x.y.z, 192.168.x.y

# Hadoop Distro Vendors
# GENERIC => User Customized Distro Vendor, GPHD => GreenPlum HD, PHD => Pivotal HD,
# HDP => Hortonworks Data Platform, CDH => Cloudera Hadoop, MAPR => MapR, BIGTOP => Apache Bigtop
serengeti.distro_vendor = GENERIC, APACHE, BIGTOP, GPHD, PHD, HDP, CDH, MAPR, KUBERNETES, MESOS

serengeti.distro_root = http://localhost/distros

# Serengeti hostname prefix, suffix hdfs, suffix mapreduce
serengeti.hostname_prefix = prefix
serengeti.hostname_suffix_hdfs = hdfs
serengeti.hostname_suffix_mapred = mapred

#Thrift server information
management.thrift.server = 127.0.0.1
management.thrift.port = 9090
management.thrift.mock = false

# task configurations
task.enable_mq = true
task.threadpool.workers = 20
task.threadpool.queue_size = 50
task.rabbitmq.host = 127.0.0.1
task.rabbitmq.port = 5672
task.rabbitmq.username =
task.rabbitmq.password =
task.rabbitmq.exchange = bddtask
task.rabbitmq.routekey_fmt = task.${task_id}
task.rabbitmq.recv_timeout_ms = 1000
task.rabbitmq.keepalive_time_ms = 10000

runtime.rabbitmq.exchange = bdd.runtime
runtime.rabbitmq.send.routekey = command

# storage size configuration (GB)
# master group represents the group contains hadoop_namenode or hadoop_jobtracker role or hbase_master role.
# worker group represents the group contains hadoop_datanode or hadoop_tasktracker role or hbase_regionserver role.
# client group represents the group contains hadoop_client or pig, hive role, or hbase_client role.
# zookeeper group represents the group contains zookeeper role.
# the last field represents the instance type, XL, L, M, and S.
storage.mastergroup.extralarge = 200
storage.mastergroup.large = 100
storage.mastergroup.medium = 50
storage.mastergroup.small = 25
storage.workergroup.extralarge = 400
storage.workergroup.large = 200
storage.workergroup.medium = 100
storage.workergroup.small = 50
storage.clientgroup.extralarge = 400
storage.clientgroup.large = 200
storage.clientgroup.medium = 100
storage.clientgroup.small = 50
storage.zookeepergroup.extralarge = 120
storage.zookeepergroup.large = 80
storage.zookeepergroup.medium = 40
storage.zookeepergroup.small = 20
# Set the system disk and swap disk controller type on cluster node to VirtualLsiLogicController or ParaVirtualSCSIController
# The system disk and swap disk will use the same controller type
storage.system_swap.disk.controller.type = VirtualLsiLogicController

# Set the default disk number for each node. This can be overridden by diskNum specified in cluster spec file.
# The total storage of a node splits into the specified number of disks.
# 0 by default which means using the default storage split policy.
# 'storage.local.disk_number_per_node' for LOCAL storage and 'storage.shared.disk_number_per_nod' for SHARED storage.
storage.local.disk_number_per_node = 0
storage.shared.disk_number_per_node = 0

elastic_runtime.automation.enable = false

# cluster.clone.service can be: simple, fast, instant
# instant means using Instant Clone (supported in vSphere 6) to clone VM during cluster creation.
cluster.clone.service = fast

# serengeti.clone.mode can be: fullclone, linkedclone
# this only takes effect when cluster.clone.service is set to simple or fast, not instant.
serengeti.clone.mode = fullclone

# Enable concurrent cluster creation. If datastores do not have enough stroage space for all the clusters being created at the same time, some of the creation will fail due to insufficient stroage space.
# If disabled, all cluster create/resize requests will be put in a queue and processed one by one.
serengeti.concurrent.job.enabled = false

# Max wait time in minutes for new cluster creation will wait for ongoing cluster creation to complete VM cloning.
serengeti.concurrent.job.maxWaitMins=240

# Use the default password for each VM instead of generate a random password.
serengeti.use.default.password = false

appmanager.types = ClouderaManager, Ambari
appmanager.factoryclass.ClouderaManager = com.vmware.bdd.plugin.clouderamgr.service.ClouderaManagerFactory
appmanager.factoryclass.Ambari = com.vmware.bdd.plugin.ambari.service.AmbariFactory
appmanager.service.connect.timeout.seconds = 30

ambari.hbase_depend_on_mapreduce = true

# milliseconds between each retry of ssh. This configuration together with the next one serengeti.ssh.max.retry.times
# is used to control how long we wait for sshd ready on the deployed vm
serengeti.ssh.sleep.time.before.retry = 3000

# 3000 * 600 = 1800,000 ms = 1800 s = 30 minutes, we set the default longest wait time to 30 minutes to handle most
# cases. If user is deploying a very large cluster, they may consider to increase serengeti.ssh.sleep.time.before.retry
# and serengeti.ssh.max.retry.times
serengeti.ssh.max.retry.times = 600

# shell exec timeout in second
usermgmt.command.exec.timeout=120

dataCollector.default.switchName = serengeti.ph.enable

# If set to true, the vm swap disk size (not the OS swap disk in the VM) will be 0KB, so as to decrease the disk space cosumption during cluster creation.
serengeti.disable.vm.swap.disk = false

# Some operation during the node VM bootup might take long time (e.g. formatting very big data disks). You can increase this timeout to give more time for these operation. 
serengeti.vm_bootup.timeout.seconds = 1800

ambari.user_run_as = root

# If you need customized sudo command such as 'pbrun', you can replace 'sudo' with proper command
serengeti.sudo.command = sudo

# enable vc requests profiler log
vc_service_profiler.log.enabled=true
#log interval in minutes
vc_service_profiler.log.interval=30

