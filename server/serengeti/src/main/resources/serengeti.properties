# serengeti bootup configurations, updated by firstboot script
serengeti.uuid = xxx-uuid

# root vm folder for all clusters will be SERENGETI-CLUSTER-${serengeti.uuid}
serengeti.root_folder_prefix = SERENGETI-vApp

# Turn on intensive checks in debug mode (including AuAssert checks)
# Note: the debug code should not have side-effect on the outside code,
# i.e. turning off debug should not leads to changes of code logic
serengeti.debug = true

# DAL transaction random rollback, i.e. deadlock simulation
# only valid when serengeti.debug = true
dal.stressTxnRollback = true

vc_datacenter = datacenter
template_id = vm-001

serengeti.distro_root = http://localhost/distros

# Turn on http proxy if the Serengeti Server needs a http proxy to connect to the Internet
# The wildcard doesn't work for 'serengeti.no_proxy'
#serengeti.http_proxy = http://proxy.domain.com:port
#serengeti.no_proxy = .domain.com,10.x.y.z, 192.168.x.y
serengeti.distro_vendor = Apache, GPHD, HDP, CDH, MAPR

query_cluster.cmd = sudo -u serengeti /home/serengeti/aurora_bigdata/distribute/sbin/ironfan_proxy.sh query :cluster_name :json_file :log_level
create_cluster.cmd = sudo -u serengeti /home/serengeti/aurora_bigdata/distribute/sbin/ironfan_proxy.sh create :cluster_name :json_file :log_level
update_cluster.cmd = sudo -u serengeti /home/serengeti/aurora_bigdata/distribute/sbin/ironfan_proxy.sh update :cluster_name :json_file :log_level
start_cluster_node.cmd = sudo -u serengeti /home/serengeti/aurora_bigdata/distribute/sbin/ironfan_proxy.sh start :nodes_name :json_file :log_level
stop_cluster_node.cmd = sudo -u serengeti /home/serengeti/aurora_bigdata/distribute/sbin/ironfan_proxy.sh stop :nodes_name :json_file :log_level
delete_cluster.cmd = sudo -u serengeti /home/serengeti/aurora_bigdata/distribute/sbin/ironfan_proxy.sh destroy :cluster_name :json_file :log_level
kill_task.cmd = sudo -u serengeti /home/serengeti/aurora_bigdata/distribute/sbin/kill_task.sh
configure_cluster.cmd = sudo -u serengeti /home/serengeti/aurora_bigdata/distribute/sbin/ironfan_proxy.sh configure :cluster_name :json_file :log_level

# task configurations
task.enable_mq = true
task.threadpool.workers = 20
task.threadpool.queue_size = 50
task.rabbitmq.host = localhost
task.rabbitmq.port = 5672
task.rabbitmq.username =
task.rabbitmq.password =
task.rabbitmq.exchange = bddtask
task.rabbitmq.routekey_fmt = task.${task_id}
task.rabbitmq.recv_timeout_ms = 1000
task.rabbitmq.keepalive_time_ms = 600000

runtime.rabbitmq.exchange = bdd.runtime
runtime.rabbitmq.send.routekey = command
runtime.rabbitmq.receive.routekey = status

# storage size configuration (GB)
# master group represents the group contains hadoop_namenode or hadoop_jobtracker role or hbase_master role.
# worker group represents the group contains hadoop_datanode or hadoop_tasktracker role or hbase_regionserver role.
# client group represents the group contains hadoop_client or pig, hive role, or hbase_client role.
# zookeeper group represents the group contains zookeeper role.
# the last field represents the instance type, XL, L, M, and S.
storage.mastergroup.extralarge = 200
storage.mastergroup.large = 100
storage.mastergroup.medium = 50
storage.mastergroup.small = 25
storage.workergroup.extralarge = 400
storage.workergroup.large = 200
storage.workergroup.medium = 100
storage.workergroup.small = 50
storage.clientgroup.extralarge = 400
storage.clientgroup.large = 200
storage.clientgroup.medium = 100
storage.clientgroup.small = 50
storage.zookeepergroup.extralarge = 120
storage.zookeepergroup.large = 80
storage.zookeepergroup.medium = 40
storage.zookeepergroup.small = 20

elastic_runtime.automation.enable = false